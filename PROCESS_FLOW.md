# תהליך עיבוד כתבות - News Balance Backend

## סקירה כללית
המערכת עובדת עם 3 סקריפטים עיקריים שמתחברים למסד נתונים PostgreSQL:

## 1. סקרפר חדשות (`filter_recent_postgres.py`)
**תפקיד:** אוסף כתבות חדשות מאתר רוטר

### תהליך:
1. **חיבור לאתר רוטר** - מתחבר לעמוד הפורום הראשי
2. **סריקת כתבות** - מוצא כל הכתבות מ-24 השעות האחרונות
3. **חילוץ תאריכים** - מחלץ תאריך ושעה מדויקים מכל כתבה
4. **סינון לפי זמן** - שומר רק כתבות מ-24 השעות האחרונות
5. **חילוץ תוכן** - נכנס לכל כתבה ומוציא את התוכן המלא
6. **ניקוי תוכן** - מסיר אלמנטים של פורום ומשאיר רק חדשות
7. **שמירה במסד נתונים** - שומר במסד PostgreSQL עם סטטוס `isprocessed = 0`

### שדות שנשמרים:
- `title` - כותרת הכתבה
- `url` - קישור לכתבה
- `content` - תוכן מלא
- `clean_content` - תוכן מנוקה
- `actual_datetime` - תאריך ושעה מדויקים
- `hash_id` - מזהה ייחודי למניעת כפילויות
- `isprocessed` - 0 (לא עובד)

## 2. מעבד כתבות (`process_articles_postgres.py`)
**תפקיד:** מעבד כתבות באמצעות AI (4 שלבים)

### תהליך 4 שלבים:

#### שלב 1: בדיקת רלוונטיות
- בודק אם הכתבה רלוונטית לנושאים פוליטיים/חברתיים
- משתמש ב-Anthropic Claude AI
- אם לא רלוונטית → `isprocessed = 2`

#### שלב 2: מחקר
- מבצע חיפוש באינטרנט על הנושא
- מחפש מקורות שונים ודעות מנוגדות
- אוסף מידע נוסף מהתקשורת הישראלית

#### שלב 3: ניתוח טכני
- יוצר ניתוח מאוזן של הנושא
- משלב את ממצאי המחקר
- מציג את כל הצדדים

#### שלב 4: כתיבה עיתונאית
- הופך את הניתוח לכתבה קריאה
- שפה עיתונאית נעימה
- טקסט זורם ללא כותרות משנה

### תוצאה:
- כתבות רלוונטיות → `isprocessed = 1` + תוכן מעובד
- כתבות לא רלוונטיות → `isprocessed = 2`

## 3. רץ ראשי (`backend_runner_postgres.py`)
**תפקיד:** מריץ את המערכת ברציפות

### תהליך:
1. **הרצת סקרפר** - כל 60 שניות
2. **הרצת מעבד** - כל 60 שניות
3. **לוגים מפורטים** - מעקב אחר התקדמות
4. **סטטיסטיקות** - דיווח על מצב המסד נתונים

## מסד נתונים PostgreSQL

### טבלה: `news_items`
```sql
- id (SERIAL PRIMARY KEY)
- title (TEXT) - כותרת
- url (TEXT UNIQUE) - קישור
- content (TEXT) - תוכן מלא
- clean_content (TEXT) - תוכן מנוקה
- actual_datetime (TEXT) - תאריך ושעה
- hash_id (TEXT UNIQUE) - מזהה ייחודי
- isprocessed (INTEGER) - סטטוס עיבוד:
  * 0 = לא עובד
  * 1 = עובד ורלוונטי
  * 2 = עובד ולא רלוונטי
- process_data (TEXT) - נתוני עיבוד AI (JSON)
- created_at (TIMESTAMP) - זמן יצירה
```

## זרימת עבודה מלאה

```
1. סקרפר → מוצא כתבות חדשות → שומר במסד (isprocessed=0)
2. מעבד → לוקח כתבות לא מעובדות → מעבד ב-AI → מעדכן (isprocessed=1/2)
3. רץ ראשי → חוזר על התהליך כל 60 שניות
```

## קבצים נדרשים
- `.env.local` - מפתחות API (ANTHROPIC_API_KEY, DATABASE_URL)
- `requirements.txt` - חבילות Python
- `Procfile` - להעלאה לרשת

## הרצה
```bash
# בדיקה מהירה
python3 filter_recent_postgres.py

# הרצה רציפה
python3 backend_runner_postgres.py
```
